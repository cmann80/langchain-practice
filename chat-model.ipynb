{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries and setting up the environmnent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the chat object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat([SystemMessage(content=\"Write the response as someone who knows next to nothing about dogs. You think that all yellow dogs are Golden Retrievers and all black dogs are pit bulls. Seriously, you don't know anything and have no clue why someone would ask you a dog question.\"), HumanMessage(content=\"Give me a fact about Belgian Malinois\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making multiple chat generation requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.generate([\n",
    "    [SystemMessage(content=\"Write the response as someone who knows next to nothing about dogs. You think that all yellow dogs are Golden Retrievers and all black dogs are pit bulls. Seriously, you don't know anything and have no clue why someone would ask you a dog question.\"), \n",
    "    HumanMessage(content=\"Give me a fact about Belgian Malinois\")],\n",
    "    [SystemMessage(content=\"Write the response as someone who loves dogs and can't think about anything else. You want to wuv them forever on their fuzzy wuzzy bellies!\"), \n",
    "    HumanMessage(content=\"Give me a fact about Belgian Malinois\")]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.llm_output)\n",
    "print(result.generations[0][0].text)\n",
    "print(result.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding extra parameters to chat request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat([SystemMessage(content=\"Write the response as someone who loves dogs and can't think about anything else. You want to wuv them forever on their fuzzy wuzzy bellies!\"), \n",
    "    HumanMessage(content=\"Give me a fact about Belgian Malinois\")], temperature =2, presence_penalty=2, max_tokens=20)\n",
    "# these are the highest possible values for temperature and presence penalty; should generate mostly gibberish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat([SystemMessage(content=\"Write the response as someone who loves dogs and can't think about anything else. You want to wuv them forever on their fuzzy wuzzy bellies!\"), \n",
    "    HumanMessage(content=\"Give me a fact about Belgian Malinois\")], temperature =0, max_tokens=20)\n",
    "# With a temperature of 0 it will generate about the same results every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the responses in a memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.predict(\"Give me a fact about Belgian Malinois\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.predict(\"Give me a fact about Belgian Malinois\")\n",
    "# For an identical prompt it should finish near instantly because it is returning a cached response rather than sending the request to the LLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
