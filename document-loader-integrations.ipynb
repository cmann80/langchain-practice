{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loader - integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=39092896\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thomastjeffery 54 minutes ago  \n",
      "             | next [â€“] \n",
      "\n",
      "I'm not convinced. The entire premise here is selection bias at its worst.Here are the steps to create an LLM:1. Tokenize the training corpus2. Train the model using the tokens3. Critique the result4. Repeat steps 2 & 3 until satisfiedWe skipped the most important step!0. Write and curate the training corpusThere is an intense focus on the training steps, but that is entirely ignorant of the elephant in the room: the original training data itself. Where did it come from? What does it mean? What is it for? What does it know?We don't train models on noise. We train them on coherent data. The very act of writing is where intelligence lies. A model can only present that intelligence. It cannot create it, and it is not aware of it. There is no objective-thought function. If there was, then the LLM could simply tell us about it.A model is like a coloring book: it doesn't matter how incoherently you stumble around, as long as you stay between the lines. The picture was not invented by coloring it: it was invented by drawing the lines.\n",
      " \n",
      "reply\n"
     ]
    }
   ],
   "source": [
    "data = loader.load()\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://news.ycombinator.com/item?id=39092896', 'title': 'New theory suggests LLMs can understand text'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0].metadata)\n",
    "# metadata has the URL and title of the post that is being commented on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a summary of a single comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = HumanMessagePromptTemplate.from_template(\"please give me a short summary of the following HackerNews Comment: \\n{comment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(chat_prompt.format_prompt(comment = data[0].page_content).to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment argues that the premise of creating a language model (LLM) is flawed due to selection bias. The commenter states that the most important step, writing and curating the training corpus, is often overlooked. They emphasize that training a model on coherent data is necessary and that a model can only present existing intelligence, not create it. The commenter compares a model to a coloring book, stating that the picture was invented by drawing the lines, not by coloring it incoherently.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
